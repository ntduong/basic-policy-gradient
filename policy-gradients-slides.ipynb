{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Practical Guide to Policy Gradients\n",
    "_Duong Nguyen_\n",
    "\n",
    "2018-11-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "\n",
    "## Reinforcement Learning Concepts\n",
    "![MDP](mdp2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- __State space__ $\\mathcal{S}$: a set of states of the environment. \n",
    "\n",
    " - state vs. observation (partial description of state)\n",
    " - fully observed vs. partially observed\n",
    "\n",
    "- __Action space__ $\\mathcal{A}$: a set of actions that an agent can select at a given state\n",
    " - discrete action spaces, e.g., board games\n",
    " - continuous action spaces, e.g., robot \n",
    "- __Policy__: specify what action $a$ for the agent to take in a given state $s$\n",
    " - Deterministic policy: $a = \\pi(s)$\n",
    " - Stochastic policy: $a \\sim \\pi(\\cdot~|~s)$, but why?\n",
    "     - naturally incorporate exploration\n",
    "     - more robust and __probably__ optimal in some settings, e.g., environments with partially observable states (e.g., card games)\n",
    "     - can converge to a deterministic policy in the limit (informally speaking...)\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Markov Decision Process (MDP) Environment__: \n",
    " - __Markov property__: $P(S_{t+1}~|~S_t, A_t) = P(S_{t+1}~|~S_t, A_t, S_{t-1},\\cdots)$, i.e., transitions only depend on the most recent state and action, and no prior history.\n",
    " \n",
    " - $\\mu_0(\\cdot)$ is the start-state distribution to sample the starting state $S_0$\n",
    " \n",
    " - __State transition__: $P(s'~|~s,a) = \\text{Prob}[S_{t+1} = s'~|~S_t = s, A_t=a]$ \n",
    " \n",
    " - __Trajectory__ (aka, rollout or episode) is a sequence of states and actions $\\tau = (s_0, a_0, s_1, a_1,\\cdots)$\n",
    "  - $s_0 \\sim \\mu_0(\\cdot)$, start-state distribution\n",
    "  - $s_{t+1} \\sim P(\\cdot~|~s_t, a_t)$ (stochastic environment) or $s_{t+1} = f(s_t, a_t)$ (deterministic environment)\n",
    "  - $a_t \\sim \\pi(\\cdot~|~s_t)$ (stochastic policy) or $a_t = \\pi(s_t)$ (deterministic policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- __Reward function__ $r: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}$, $r_t = r(s_t,a_t,s_{t+1})$ \n",
    "     - other definitions in the literature: $r(s_t)$, $r(s_t,a_t)$\n",
    "     - scalar _feedback_ signal to facilitate agent learning\n",
    "     - reward function design is difficult, e.g., self-driving car\n",
    "         -  an important research topic, e.g., [inverse RL](https://arxiv.org/abs/1806.06877)\n",
    "\n",
    "- __Discount factor__: $0 < \\gamma < 1$: discount the present value of future rewards\n",
    "\n",
    "`Reference`\n",
    "- The Sutton+Barto [book](http://incompleteideas.net/book/the-book-2nd.html) Chapter 3\n",
    "- David Silver's [lecture 2](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology\n",
    "\n",
    "* __Episodic vs. Continuing__\n",
    "* __Return__: cumulative rewards\n",
    "\n",
    " - $G_t = r_{t} + \\gamma r_{t+1} + \\cdots = \\sum_{t'=t}^{\\infty~\\textrm{or}~T} \\gamma^{t'-t}r_{t'}$, $G_t = r_{t} + \\gamma G_{t+1}$\n",
    " \n",
    " - For a trajectory $\\tau$, \n",
    "     - finite horizon undiscounted return: $G(\\tau) = \\sum_{t=0}^T r_t$\n",
    "     - infinite horizon discounted return: $G(\\tau) = \\sum_{t=0}^\\infty \\gamma^t r_t$\n",
    "  \n",
    "* __On-policy value functions__: For a policy $\\pi$,\n",
    " - state-value function:\n",
    " \n",
    "     - $V_\\pi (s) = \\mathbb{E}[G_t~|~S_t = s; \\pi] = \\mathbb{E}[r_{t} + \\gamma V_\\pi (S_{t+1})~|~S_t = s; \\pi]$\n",
    "\n",
    " - action-state-value function:\n",
    " \n",
    "     - $Q_\\pi(s, a) = \\mathbb{E}[G_t~|~S_t=s, A_t=a; \\pi] = \\mathbb{E}[r_{t} + \\gamma V_\\pi (S_{t+1})~|~S_t = s, A_t = a]$\n",
    "     \n",
    " - __Advantage function__ $A_\\pi(s,a) := Q_\\pi(s,a) - V_\\pi(s)$, i.e., how much better an action $a$ is than others on average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Bellman equations__:\n",
    "\n",
    " - $V_\\pi(s) = \\mathbb{E}_{a \\sim \\pi, s' \\sim P}\\left[r(s,a,s') + \\gamma V_\\pi(s') \\right]$\n",
    " \n",
    " - $Q_\\pi(s,a) = \\mathbb{E}_{s' \\sim P} \\left[r(s,a,s') + \\gamma \\mathbb{E}_{a' \\sim \\pi} Q_\\pi(s', a')\\right]$\n",
    " \n",
    "The __Bellman backup__ (for a state, or state-action pair) refers to the right-hand side of the Bellman equation, the reward-plus-next-value.\n",
    "\n",
    "Also, $V_\\pi(s) = \\mathbb{E}_{a \\sim \\pi} Q_\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Optimal value functions__\n",
    "\n",
    "- $V^{*}(s) = \\underset{\\pi}{\\text{max}} V_\\pi(s)$\n",
    "\n",
    "- $Q^{*}(s,a) = \\underset{\\pi}{\\text{max}} Q_\\pi(s,a)$\n",
    "\n",
    "- $V^{*}(s) = \\text{max}_{a} Q^{*}(s,a)$\n",
    "\n",
    "__Exercise__: Write Bellman equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## RL Algorithms Taxonomy\n",
    "\n",
    "![openai-taxonomy](rl_algo_taxonomy.svg)\n",
    "\n",
    "[Source](http://spinningup.openai.com/en/latest/spinningup/rl_intro2.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Optimization\n",
    "\n",
    "![Big picture](policy_improvement_landscape.png)\n",
    "\n",
    "[Source](https://media.nips.cc/Conferences/2016/Slides/6198-Slides.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Parameterized policy__ with parameters $\\theta \\in \\mathbb{R}^d$,\n",
    "\n",
    "- $\\pi_\\theta(a~|~s)$ (stochastic)\n",
    "- $\\pi_\\theta(s)$ (deterministic)\n",
    "\n",
    "\n",
    "__Objective function__: $J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[G(\\tau)\\right]$\n",
    "\n",
    "__Optimization__:\n",
    "$$\\theta^{*} = \\underset{\\theta \\in \\mathbb{R}^d}{\\text{ argmax }} J(\\theta),$$\n",
    "which can be solved with:\n",
    "- __Gradient ascent__\n",
    "- Gradient-free/Blackbox optimization, e.g., hill-climbing, genetic algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "- In episodic setting: $J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[V(s_0)] = \\mathbb{E}_{\\pi_\\theta}[\\sum_{t=0}^T \\gamma^t R_t]$, i.e., the __value of the start state__\n",
    "\n",
    "- In continuing setting: \n",
    " \n",
    " - $J(\\theta) = \\sum_s d^{\\pi}(s) V_\\pi(s)$, i.e., the (weighted) __average state-value__ weighted by the limiting (stationary) state distribution induced by the policy $\\pi_\\theta$, $d^{\\pi}(s) = \\sum_{s'}P_\\pi(s',s) d^{\\pi}(s')$\n",
    " \n",
    " - $J(\\theta) = \\sum_s d^{\\pi}(s) \\sum_a \\pi(a~|~s)R(s,a)$, i.e., the __average reward per timestep__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Ascent\n",
    "\n",
    "Find a local maximum of the objective $J(\\theta)$:\n",
    "$\\theta^{\\text{next}} \\leftarrow \\theta^{\\text{cur}} + \\alpha \\nabla_\\theta J(\\theta)_{|\\theta = \\theta^{cur}},$\n",
    "\n",
    "where:\n",
    "- policy gradient: $\\nabla_\\theta J(\\theta) = \\left[\\partial J/\\partial \\theta_1,\\cdots, \\partial J/\\partial \\theta_d\\right]^T$, \n",
    "- step-size (learning rate): $\\alpha > 0$\n",
    "\n",
    "$\\rightarrow$ the main problem is __to compute the policy gradient__!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[__Finite difference method__](https://en.wikipedia.org/wiki/Finite_difference_method)\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_i} \\approx \\frac{J(\\theta + \\epsilon u^{(i)}) - J(\\theta)}{\\epsilon},$$\n",
    "where $u^{(i)}$ ($i=1,\\cdots,d$) is the d-dimensional unit vector with $u^{(i)}_i = 1, u^{(i)}_j = 0$ for $j \\neq i$\n",
    "\n",
    "__But can we compute the gradient analytically?__ $\\rightarrow$ Yes, Policy Gradient methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Policy Gradients\n",
    "\n",
    "__Can we compute the policy gradient analytically?__ \n",
    "- Yes, use the [log-derivative trick](http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/), assumed that the policy is differentiable w.r.t the parameters $\\theta$.\n",
    "\n",
    "### Warm-up: One-step MDP\n",
    "$$J(\\theta) = \\mathbb{E}_{s \\sim \\mu_0(\\cdot),~a \\sim \\pi_\\theta(\\cdot~|~s)}\\left[r(s,a)\\right] = \\sum_s \\mu_0(s) \\sum_a \\pi_\\theta(a~|~s) r(s,a)$$\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\sum_s \\mu_0(s) \\sum_a \\nabla_\\theta \\pi_\\theta(a~|~s) r(s,a)$$\n",
    "\n",
    "Applying the log-derivative trick,\n",
    "$\\nabla_\\theta \\pi_\\theta(a~|~s) = \\pi_\\theta(a~|~s)\\frac{\\nabla_\\theta \\pi_\\theta(a~|~s)}{\\pi_\\theta(a~|~s)} = \\pi_\\theta(a~|~s) \\nabla_\\theta\\log \\pi_\\theta(a~|~s)$,\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\mu_0(\\cdot),~a \\sim \\pi_\\theta(\\cdot~|~s)}[\\nabla_\\theta\\log \\pi_\\theta(a~|~s) r(s,a)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-step MDP\n",
    "\n",
    "![Episodic process](episodic.png)\n",
    "\n",
    "\n",
    "_Consider episodic setting with finite-horizon undiscounted return, but a similar derivation applies to continuing setting with infinite-horizon discounted return_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Trajectory $\\tau = (s_0, a_0, s_1, \\cdots, s_{T-1}, a_{T-1}, s_T)$, which is sampled with the following process:\n",
    "\n",
    "- $s_0 \\sim \\mu_0(\\cdot),~a_0 \\sim \\pi_\\theta(\\cdot~|~s_0),~s_1 \\sim P(\\cdot~|~s_0, a_0)$\n",
    "\n",
    "- $s_{t+1} \\sim P(\\cdot~|~s_{t}, a_{t})$ for $t=0,\\cdots,T-1$\n",
    "\n",
    "- $s_T$ is a terminal state, ending the trajectory\n",
    "\n",
    "__Probability of a trajectory given a policy $\\pi_\\theta$__:\n",
    "\n",
    "$$P(\\tau; \\pi_\\theta) = \\mu_0(s_0)\\prod_{t \\geq 0}\\pi_\\theta(a_t~|~s_t)P(s_{t+1}~|~s_t, a_t)$$\n",
    "$$\\log P(\\tau; \\pi_\\theta) = \\log\\mu_0(s_0) + \\sum_{t\\geq 0}\\log P(s_{t+1}~|~s_t, a_t) + \\sum_{t\\geq 0}\\log\\pi_\\theta(a_t~|~s_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Policy Gradient Derivation\n",
    "\n",
    "$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[G(\\tau)\\right] = \\int_\\tau G(\\tau)P(\\tau; \\pi_\\theta) d\\tau$\n",
    "\n",
    "$\n",
    "\\begin{matrix}\n",
    " \\nabla_\\theta J(\\theta) & = &\\nabla_\\theta \\int_\\tau G(\\tau)P(\\tau; \\pi_\\theta) d\\tau = \\int_\\tau G(\\tau) P(\\tau; \\pi_\\theta) \\frac{\\nabla_\\theta P(\\tau; \\pi_\\theta)}{P(\\tau; \\pi_\\theta)} d\\tau \\\\ \n",
    " & = & \\int_\\tau G(\\tau) P(\\tau; \\pi_\\theta) \\nabla_\\theta \\log P(\\tau; \\pi_\\theta) d\\tau = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[G(\\tau) \\nabla_\\theta \\log P(\\tau; \\pi_\\theta)\\right] \n",
    "\\end{matrix}\n",
    "$\n",
    "\n",
    "Note: \n",
    "$\n",
    "\\log p(\\tau; \\pi_\\theta) = \\log\\mu_0(s_0) + \\sum_{t\\geq 0}\\log P(s_{t+1}~|~s_t, a_t) + \\sum_{t\\geq 0}\\log\\pi_\\theta(a_t~|~s_t)\n",
    "$\n",
    "\n",
    "$\\rightarrow \\nabla_\\theta \\log p(\\tau; \\pi_\\theta) = \\sum_{t\\geq 0}\\nabla_\\theta \\log\\pi_\\theta(a_t~|~s_t)$\n",
    "\n",
    "__Policy Gradient__: \n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[G(\\tau)\\sum_{t\\geq 0}\\nabla_\\theta \\log\\pi_\\theta(a_t~|~s_t)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Estimator\n",
    "\n",
    "- Collect trajectories $\\{\\tau^{(i)}\\}_{i=1}^N$, and estimate the gradient with sample mean:\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N \\left[G(\\tau^{(i)})\\sum_{t\\geq 0}\\nabla_\\theta\\log\\pi_\\theta(a_t^{(i)}~|~s_t^{(i)})\\right] = \\frac{1}{N}\\sum_{i=1}^N \\left[(\\sum_{t \\geq 0}r_{t}^{(i)})\\sum_{t\\geq 0}\\nabla_\\theta\\log\\pi_\\theta(a_t^{(i)}~|~s_t^{(i)})\\right]~~(1)$$\n",
    "\n",
    "c.f. Maximum likelihood objective when predicting actions from states with samples $\\{s^{(i)}_t, a^{(i)}_t\\}_{t\\geq 0, i=1,\\cdots,N}$:\n",
    "\n",
    "$$\\nabla_\\theta J_{MLE}(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N \\left[\\sum_{t\\geq 0}\\nabla_\\theta\\log\\pi_\\theta(a_t^{(i)}~|~s_t^{(i)})\\right]$$\n",
    "\n",
    "Intuitively, the PG objective can be seen as the _weighted_ MLE, in which\n",
    "- a _good_ episode with high return $r(\\tau)$ is made more likely; this basically increases the probabilities of the actions taken in the episode\n",
    "- a _bad_ episode with low return is made less likely; and in turns the probabilities of the actions taken in the episode are pushed down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Estimator (2)\n",
    "\n",
    "$$\\widehat{g} = \\frac{1}{N}\\sum_{i=1}^N \\left[\\sum_{t \\geq 0}r_{t}^{(i)}\\sum_{t\\geq 0}\\nabla_\\theta\\log\\pi_\\theta(a_t^{(i)}~|~s_t^{(i)})\\right]$$\n",
    "\n",
    "- Unbiased\n",
    "- High variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variance Reduction\n",
    "\n",
    "$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[G(\\tau)\\sum_{t\\geq 0}\\nabla_\\theta \\log\\pi_\\theta(a_t~|~s_t)\\right] = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t\\geq 0}\\nabla_\\theta \\log\\pi_\\theta(a_t~|~s_t)\\left(\\sum_{t\\geq 0} r_t\\right)~\\right]$\n",
    "\n",
    "### Reward-To-Go\n",
    "__Idea__: only rewards obtained after taking an action should be used to evaluate how good the action is,\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t\\geq 0}\\nabla_\\theta \\log\\pi_\\theta(a_t~|~s_t)\\left(\\sum_{t'\\geq t} r_{t'}\\right)\\right]$$\n",
    "\n",
    "[Proof](http://spinningup.openai.com/en/latest/spinningup/extra_pg_proof1.html)\n",
    "\n",
    "Why does it reduce variance?\n",
    "- Intuitively, the sum of fewer random terms has smaller variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discounting future rewards\n",
    "\n",
    "__Idea__: Discount future rewards to further reduce their values when evaluating the current action $\\rightarrow$ introduce bias to reduce variance\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t\\geq 0}\\nabla_\\theta \\log\\pi_\\theta(a_t~|~s_t)\\left(\\sum_{t'\\geq t} \\gamma^{t'-t} r_{t'}\\right)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vanilla PG (aka REINFORCE)\n",
    "![Vanilla PG](reinforce2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Baseline Subtraction\n",
    "\n",
    "__Idea__: _are you doing better than baseline_\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t\\geq 0}\\nabla_\\theta \\log\\pi_\\theta(a_t~|~s_t)\\left(\\sum_{t'\\geq t} r_{t'} - b(s_t) \\right)\\right]$$\n",
    "\n",
    "How much the sample return is better than some baseline $b(s_t)$ (e.g., the average return)\n",
    "- $b(s_t) = V_\\pi(s_t)$\n",
    "\n",
    "__Why does subtracting baseline reduce variance?__\n",
    " - See this [paper](http://jmlr.csail.mit.edu/papers/volume5/greensmith04a/greensmith04a.pdf) for rigorous proof\n",
    " - Whiteboard exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# REINFORCE with baseline\n",
    "![Vanilla PG with baseline](reinforce-baseline2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Yet another PG formula\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t\\geq 0}\\nabla_\\theta \\log\\pi_\\theta(a_t~|~s_t)~Q_\\pi(s_t, a_t)\\right]$$\n",
    "\n",
    "[Proof](https://spinningup.openai.com/en/latest/spinningup/extra_pg_proof2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Policy Gradient Theorems\n",
    "\n",
    "#### Stochastic PG \n",
    "\n",
    "[Paper](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta\\log \\pi_\\theta(a~|~s)Q_{\\pi_\\theta}(s,a)]$$\n",
    "\n",
    "#### Deterministic PG\n",
    "\n",
    "[Paper](http://proceedings.mlr.press/v32/silver14.pdf)\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\pi_\\theta(s)\\nabla_a Q_{\\pi_\\theta}(s, a)_{|a = \\pi_\\theta(s)}]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problems with vanilla PGs\n",
    "\n",
    "- __High variance thus unstable and slow convergence__\n",
    " - __Combine with value function approximation, e.g., advantage actor-critic__\n",
    " \n",
    "- Sample intensive\n",
    "\n",
    "- Credit assignment is difficult\n",
    "\n",
    "- How to select reasonable step-size?\n",
    " - TRPO, PPO (Advanced topics, see e.g., [NIPS 2016 tutorial](https://nips.cc/Conferences/2016/Schedule?showEvent=6198))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Formula with Advantage Function\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta\\log \\pi_\\theta(a~|~s)~A_{\\pi_\\theta}(s,a)]$$\n",
    "\n",
    "\n",
    "- __Advantage function__: $A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)$\n",
    "  - __How good the action $a$ is w.r.t the average__\n",
    "\n",
    "\n",
    "- __Why?__\n",
    "    - $\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta\\log \\pi_\\theta(a~|~s)Q_{\\pi_\\theta}(s,a)] = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta\\log \\pi_\\theta(a~|~s)(Q_{\\pi_\\theta}(s,a) - V_{\\pi_\\theta}(s))]$\n",
    "    - Subtracting state-dependent baseline $V_{\\pi_\\theta}(s)$ is still unbiased\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (Online) Actor-Critic \n",
    "\n",
    "Can estimate advantage function with full sampled episodes (Monte Carlo approach)\n",
    "\n",
    "But, __do we need full episodes to evaluate the advantage function?__\n",
    "\n",
    "- No, use value function approximation, e.g., $Q_\\pi(s, a) = Q(s,a; \\boldsymbol{w_Q}), V_\\pi(s) = V(s; \\boldsymbol{w_V})$ \n",
    "- The idea is to update the value function approximator $\\boldsymbol{w_Q}, \\boldsymbol{w_V}$ with each interaction ${(s_t, a_t, s_{t+1}, r_{t})}$\n",
    "\n",
    "### Intuition\n",
    "- Approximate learning with supervision\n",
    "- Actor: policy\n",
    "- Critic: \n",
    "    - evaluate the actor action at each timestep\n",
    "    - \"re-weight\" policy gradient \n",
    "- Both actor and critic update online after each interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temporal Difference (TD) Error\n",
    "\n",
    "__Estimate advantage function with TD error__\n",
    "\n",
    "$\\delta_t = r_t + \\gamma V_{\\pi_\\theta}(s_{t+1}) - V_{\\pi_\\theta}(s_t)$\n",
    "\n",
    "$\\delta_{\\pi_\\theta} = \\mathbb{E}_{\\pi_\\theta}[\\delta_t] = \\mathbb{E}_{\\pi_\\theta}[r_t + \\gamma V_{\\pi_\\theta}(s_{t+1}) - V_{\\pi_\\theta}(s_t)] = Q_{\\pi_\\theta}(s,a) - V_{\\pi_\\theta}(s) = A_{\\pi_\\theta}(s,a)$\n",
    "\n",
    "$\\rightarrow$ Can use TD error for the gradient:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta\\log \\pi_\\theta(a~|~s)~\\delta_{\\pi_\\theta}]$$\n",
    "\n",
    "$\\rightarrow$ Estimate the TD error with __one__ value function approximation, $V(\\cdot, w_V)$, \n",
    "\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}; \\boldsymbol{w}) - V(s_t; \\boldsymbol{w})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advantage Actor-Critic PG with one-step TD\n",
    "\n",
    "![One-step Actor-Critic](ac-td1.png)\n",
    "\n",
    "- __Work with continuing tasks since it doesn't require full episode__\n",
    "- __Trade bias for variance reduction due to value function bootstrapping__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## N-step TD Error\n",
    "\n",
    "Middleground between one-step TD and Monte Carlo (~all-step TD)\n",
    "\n",
    "$\\delta^{(n)}_t = r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{n}r_{t+n} - V(s_{t}; \\boldsymbol{w_V})$\n",
    "\n",
    "- small $n$: low variance, but high bias\n",
    "- large $n$: high variance, but low bias\n",
    "\n",
    "__Advanced__: Combine all $n$-step TD errors into one (~ $TD(\\lambda)$ idea) $\\rightarrow$ [Generalized Advantage Estimator (GAE - John Schulman+etal)](https://arxiv.org/abs/1506.02438)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantage Actor-Critic with n-step TD\n",
    "\n",
    "![A2C](a2c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "\n",
    "__Policy Gradient is basically Gradient Ascent__ with:\n",
    " - Techniques to reduce variance and improve stability\n",
    " - Techniques to improve sample efficiency\n",
    " \n",
    "__Various gradient formula:__ $\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t\\geq 0}\\nabla_\\theta \\log\\pi_\\theta(a_t~|~s_t)~\\Phi_t\\right]$\n",
    " - Replace $\\Phi_t$ with $G_t$, $Q_\\pi, A_\\pi, \\delta_\\pi$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced PGs\n",
    "\n",
    "1. [TRPO](https://arxiv.org/abs/1502.05477) $\\rightarrow$ [PPO](https://arxiv.org/abs/1707.06347)\n",
    "See also, e.g., this [NIPS 2016 tutorial](https://nips.cc/Conferences/2016/Schedule?showEvent=6198)\n",
    "\n",
    "2. [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n",
    "Off-policy actor-critic for continuous actions \n",
    "\n",
    "3. State-action dependent baseline methods, e.g., [QProp](https://arxiv.org/abs/1611.02247)\n",
    "\n",
    "I'll probably write short blog posts or slides about these topics in the future. Stay tuned :)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
